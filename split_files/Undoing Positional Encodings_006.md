---
file: /home/pedro/Documents/chatgpt_export/Markdown/Undoing Positional Encodings.md
heading_stack: <root> -> 4e44ee95-cb7a-4242-97be-52b6794098fd -> System -> 2984f899-ca70-4b1e-b2be-96ced8112a5e -> System -> aaa2f123-87f4-49f7-b9f0-234481819e63 -> User
---
# User

Is it possible to "undo" positional encodings in large language models in order to get just an "untainted" language model hidden state representation?

[child ⬇️](#f32a251d-43c1-468d-8d74-d4e3d39545d5)

---

(main branch ⎇)
###### f32a251d-43c1-468d-8d74-d4e3d39545d5
[parent ⬆️](#aaa2f123-87f4-49f7-b9f0-234481819e63)
