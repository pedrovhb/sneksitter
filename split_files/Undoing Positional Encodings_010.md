---
file: /home/pedro/Documents/chatgpt_export/Markdown/Undoing Positional Encodings.md
heading_stack: <root> -> 4e44ee95-cb7a-4242-97be-52b6794098fd -> System -> 2984f899-ca70-4b1e-b2be-96ced8112a5e -> System -> aaa2f123-87f4-49f7-b9f0-234481819e63 -> User -> f32a251d-43c1-468d-8d74-d4e3d39545d5 -> Assistant -> aaa24e29-3f25-43e5-8bec-450e86dff908 -> User
---
# User

My goal is to get a visualization of different branches of generation across latent space by umapping latents, but it seems they follow mostly the same path regardless of contents; I suspect it's position encoding messing with it. How could I take that out of the equation?

[child ⬇️](#777c7829-2f03-4e21-a4d2-d49947300f77)

---

(main branch ⎇)
###### 777c7829-2f03-4e21-a4d2-d49947300f77
[parent ⬆️](#aaa24e29-3f25-43e5-8bec-450e86dff908)
